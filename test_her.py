# GCNQ: Multi Q
import numpy as np
import networkx as nx
import random, io
from dqn import DQNTrainer
from gengraph import random_sbm
from change_baseline import Change
from test_env_net_delete import NetworkEnv
from goal import NetworkEnv2
from deepwalk import DeepWalk

import matplotlib.pyplot as plt

import torch

from torch.utils.tensorboard import SummaryWriter

from implementation_alg import Buffer, PriortizedReplay
from implementation_alg import OrnsteinUhlenbeckActionNoise

from influence import influence
import pickle, os
import influence as infl
import gc
import logging, argparse
from matplotlib import pyplot as plt
import time

g_paths = [
    # 'data/rt/copen.pkl',
    # 'data/rt/assad.pkl',
    # 'data/rt/damascus.pkl',
    # 'data/rt/israel.pkl',
    # 'data/rt/obama.pkl',
    # 'data/rt/tlot.pkl',
    # 'data/rt/voteonedirection.pkl',
    # 'data/rt/occupy.pkl'
    # 'data/mammal/bhp.pkl',
    'data/mammal/kcs.pkl'

]

acmodel = torch.load('model_test/mammal/CHER_sample_form1_9900.pth')
# acmodel = torch.load('model_test/CHER_sample_form1_9900.pth')
# acmodel = torch.load('delete/delete_loss9900.pth')

syn = False
ratio = 5
os.makedirs("models", exist_ok=True)
os.makedirs("logs", exist_ok=True)


def arg_parse():
    parser = argparse.ArgumentParser(description='Influence Maxima Arguments')
    parser.add_argument('--logfile', dest='logfile', type=str, default='test_her_form2_1200_isreal.log',
                        help='Logging file')
    parser.add_argument('--logdir', dest='logdir', type=str, default=None,
                        help='Tensorboard LogDir')
    parser.add_argument('--log-level', dest='loglevel', type=int, default=2, choices=[1, 2],
                        help='Logging level')
    # parser.add_argument('--sample-budget', dest='budget', type=int, default=10,
    #                     help='Number of queries for sampling')
    parser.add_argument('--sample-budget', dest='budget', type=int, default=5,
                        help='Number of queries for sampling')
    parser.add_argument('--extra-seeds', dest='extra_seeds', type=int, default=5,
                        help='Initial number of random seeds')
    parser.add_argument('--prop-prob', dest='prop_probab', type=float, default=0.1,
                        help='Propogation Probability for each Node')
    parser.add_argument('--cpu', dest='cpu', type=int, default=5,
                        help='Number of CPUs to use for influence sampling')
    parser.add_argument('--samples', dest='samples', type=int, default=100,
                        help='Number of samples in Influence Maximization')
    parser.add_argument('--opt', dest='obj', type=float, default=0,
                        help='Threshold for reward')
    parser.add_argument('--infl-budget', dest='ibudget', type=int, default=10,
                        help='Number of queries during influence(greedy steps)')
    parser.add_argument('--render', dest='render', type=int, default=0,
                        help='1 to Render graphs, 0 to not')
    parser.add_argument('--write', dest='write', type=int, default=1,
                        help='1 to write stats to tensorboard, 0 to not')

    parser.add_argument('--change-seeds', dest='changeSeeds', type=int, default=1,
                        help='1 to change seeds after each episode, 0 to not')
    parser.add_argument('--add-noise', dest='add_noise', type=int, default=1,
                        help='1 to add noise to action 0 to not')
    parser.add_argument('--sep-net', dest='sep_net', type=int, default=0,
                        help='Seperate network rep for actor and critic')

    parser.add_argument('--save-freq', dest='save_every', type=int, default=100,
                        help='Model save frequency')
    parser.add_argument('--eps', dest='num_ep', type=int, default=10000,
                        help='Number of Episodes')

    parser.add_argument('--buffer-size', dest='buff_size', type=int, default=4000,
                        help='Replay buffer Size')
    parser.add_argument('--gcn_layers', dest='gcn_layers', type=int, default=2,
                        help='No. of GN Layers before each pooling')
    parser.add_argument('--num_poolig', dest='num_pooling', type=int, default=1,
                        help='No.pooling layers')
    parser.add_argument('--assign_dim', dest='assign_dim', type=int, default=100,
                        help='pooling hidden dims 1')
    parser.add_argument('--assign_hidden_dim', dest='assign_hidden_dim', type=int, default=150,
                        help='pooling hidden dims 2')

    parser.add_argument('--actiondim', dest='action_dim', type=int, default=40,
                        help='Action(Node) Dimensions')
    parser.add_argument('--const_features', dest='const_features', type=int, default=0,
                        help='1 to have constant features')
    parser.add_argument('--inputdim', dest='input_dim', type=int, default=21,
                        help='Node features Dimensions')

    parser.add_argument('--step_reward', dest='nop_reward', type=float, default=0,
                        help='Reward for each step')
    parser.add_argument('--bad_reward', dest='bad_reward', type=float, default=0,
                        help='Reward for each step that is closer to active')
    parser.add_argument('--norm_reward', dest='norm_reward', type=int, default=0,
                        help='Normalize reward with opt')
    parser.add_argument('--max_reward', dest='max_reward', type=int, default=None,
                        help='Normalize reward with opt')
    parser.add_argument('--min_reward', dest='min_reward', type=int, default=None,
                        help='Normalize reward with opt')

    parser.add_argument('--lr', dest='lr', type=float, default=1e-4,
                        help='Learning Rate')
    parser.add_argument('--eta', dest='eta', type=float, default=0.1,
                        help='Target network transfer rate')
    parser.add_argument('--gamma', dest='gamma', type=float, default=0.99,
                        help='Discount rate')
    # parser.add_argument('--epsilon', dest='epsilon', type=float, default=0.1,
    #                     help='Epsilon exploration')
    parser.add_argument('--epsilon', dest='epsilon', type=float, default=0,
                        help='Epsilon exploration')
    parser.add_argument('--batch_size', dest='batch_size', type=int, default=100,
                        help='Gradient Update Batch Size')

    parser.add_argument('--use_cuda', dest='use_cuda', type=int, default=1,
                        help='1 to use cuda 0 to not')
    parser.add_argument('--walk_len', dest='walk_len', type=int, default=10,
                        help='Walk Length')

    parser.add_argument('--num_walks', dest='num_walks', type=int, default=80,
                        help='Walk Length')
    parser.add_argument('--win', dest='win', type=int, default=5,
                        help='Window size')
    parser.add_argument('--emb_iters', dest='emb_iters', type=int, default=50,
                        help='Walk Length')

    parser.add_argument('--noise_momentum', dest='noise_momentum', type=float, default=0.15,
                        help='Noise Momentum')
    parser.add_argument('--noise_magnitude', dest='noise_magnitude', type=float, default=0.2,
                        help='Noise Magnitude')

    parser.add_argument('--noise_decay', dest='noise_decay_rate', type=float, default=0.999,
                        help='Noise Decay Rate')
    parser.add_argument('--eta_decay', dest='eta_decay', type=float, default=1.,
                        help='eta Decay Rate')
    parser.add_argument('--alpha_decay', dest='alpha_decay', type=float, default=1.,
                        help='alpha Decay Rate')
    parser.add_argument('--eps_decay', dest='eps_decay_rate', type=float, default=0.999,
                        help='Epsilon Decay Rate')

    parser.add_argument('--sample_times', dest='times_mean', type=int, default=10,
                        help='Number of times to sample objective from fluence algorithm')
    parser.add_argument('--sample_times_env', dest='times_mean_env', type=int, default=3,
                        help='Number of times to sample objective from fluence algorithm for env rewards')
    parser.add_argument('--save_model', dest='save_model', type=str, default='HER_sample_',
                        help='Name of Save model')
    parser.add_argument('--neigh', dest='k', type=int, default=1,
                        help='K nearest for ation')
    #
    # parser.add_argument('--min_goal', dest='min_goal', type=int, default=20,
    #                     help='Minimum desired goal')
    # parser.add_argument('--mid_goal', dest='mid_goal', type=int, default=40,
    #                     help='Middle desired goal')
    # parser.add_argument('--max_goal', dest='max_goal', type=int, default=60,
    #                     help='Maximum desired goal')
    return parser.parse_args()


random.seed(10)
args = arg_parse()
rg = np.random.RandomState(10)
rg1 = np.random.RandomState(10)
# n = 100
logfile = args.logfile

logging.basicConfig(level=args.loglevel * 10, filename=logfile, filemode='w', datefmt='%d-%b-%y %H:%M:%S',
                    format='%(levelname)s - %(asctime)s - %(message)s ')

budget = args.budget
extra_seeds = args.extra_seeds

infl.PROP_PROBAB = args.prop_probab
infl.BUDGET = args.ibudget

from multiprocessing import cpu_count

import multiprocessing
# multiprocessing.freeze_support()

infl.PROCESSORS = cpu_count() if args.cpu <= 0 else args.cpu

infl.SAMPLES = args.samples
print('Samples icm:', infl.SAMPLES)
render = args.render
write = args.write

changeSeeds = args.changeSeeds

add_noise = args.add_noise

debug = False

save_every = args.save_every

NUM_EP = args.num_ep
BUFF_SIZE = args.buff_size
action_dim = args.action_dim
if args.const_features:
    input_dim = args.input_dim
else:
    input_dim = args.action_dim + 3
nop_reward = args.nop_reward

LR = args.lr
eta = args.eta
batch_size = args.batch_size
gcn_layers = args.gcn_layers
num_pooling = args.num_pooling
assign_dim = args.assign_dim
assign_hidden_dim = args.assign_hidden_dim

use_cuda = args.use_cuda

noise_momentum = args.noise_momentum
noise_magnitude = args.noise_magnitude

noise_decay_rate = args.noise_decay_rate
eta_decay = args.eta_decay

times_mean = args.times_mean

noise_param = 1


# generate graph
graphs = []
for g_path in g_paths:
    with open(g_path, 'rb') as fl:
        graphs.append(pickle.load(fl))
    print(g_path)
    g = graphs[-1]
    print("Nodes:", len(g))
    print("Edges:", len(g.edges))
    logging.info("Nodes: " + str(len(g)) + ' Edges: ' + str(len(g.edges)))

if args.logdir is None:
    writer = SummaryWriter()
else:
    writer = SummaryWriter(os.path.join('runs_her2', args.logdir))

# Get best baseline
opts = []
for gp, g in zip(g_paths, graphs):
    opt_obj, local_obj, S_opt = influence(g, g)
    print(gp)
    print('OPT Results:', opt_obj, S_opt)
    logging.info('OPT Results:' + str(opt_obj) + ' ' + str(S_opt))
    opts.append(opt_obj)

# Initialize seeds
e_seeds_list = []
for g in graphs:
    e_seeds_list.append(list(rg.choice(len(g), extra_seeds)))
# e_seeds = [31, 171]

# logging.debug('Extra Seeds:' + str(e_seeds_list))
ch = []
for gp, g in zip(g_paths, graphs):
    rs = []
    for _ in range(5):
        change = Change(g, budget=budget * 2, seeds=[])
        obj1, local_obj1, S1 = change()
        rs.append(obj1)
    ch.append(np.mean(rs))
    print("Change for %s is %f" % (gp, ch[-1]))
logging.info('Change Results:' + str(obj1) + ' ' + str(S1))

final_goal = 0

if args.obj is not None:
    obj = args.obj

envs = []
for g, seeds in zip(graphs, e_seeds_list):
    env = NetworkEnv(fullGraph=g, seeds=seeds, opt_reward=0, nop_r=args.nop_reward,
                     times_mean=args.times_mean_env, bad_reward=args.bad_reward, clip_max=args.max_reward,
                     clip_min=args.min_reward, normalize=args.norm_reward)
    envs.append(env)
# replay = PriortizedReplay(BUFF_SIZE, 10, beta=0.6)

goal_envs = []
for g, seeds in zip(graphs, e_seeds_list):
    env = NetworkEnv2(fullGraph=g, seeds=seeds, opt_reward=0, nop_r=args.nop_reward,
                     times_mean=args.times_mean_env, bad_reward=args.bad_reward, clip_max=args.max_reward,
                     clip_min=args.min_reward, normalize=args.norm_reward, budget=budget)
    goal_envs.append(env)

logging.info('State Dimensions: ' + str(action_dim))
logging.info('Action Dimensions: ' + str(action_dim))

# acmodel = DQNTrainer(input_dim=input_dim, state_dim=action_dim, action_dim=action_dim, replayBuff=replay, lr=LR,
#                      use_cuda=use_cuda, gamma=args.gamma,
#                      eta=eta, gcn_num_layers=gcn_layers, num_pooling=num_pooling, assign_dim=assign_dim,
#                      assign_hidden_dim=assign_hidden_dim)

noise = OrnsteinUhlenbeckActionNoise(action_dim, theta=noise_momentum, sigma=noise_magnitude)

# ! Doesn't Support nested models
# writer.add_graph(acmodel.actor_critic)
rws = []


def make_const_attrs(graph, input_dim):
    n = len(graph)
    mat = np.ones((n, input_dim))
    # mat = np.random.rand(n,input_dim)
    return mat


def make_env_attrs(n=len(g), input_dim=input_dim, env=env):
    mat1 = np.ones((n, int(input_dim / 2)))
    mat2 = np.ones((n, int(input_dim / 2)))
    mat1[list(env.active), :] = -1
    mat2[list(env.possible_actions), :] = -1
    return np.concatenate((mat1, mat2), 1)


# def make_env_attrs_1(env, embs, n=len(g), input_dim=input_dim):
#     mat1 = np.zeros((n, int(action_dim + 2)))
#     for u in env.active:
#         mat1[u, :-2] = embs[u]
#         mat1[u, -2] = 1
#     for u in env.possible_actions:
#         mat1[u, :-2] = embs[u]
#         mat1[u, -1] = 1
#     return mat1


def make_env_attrs_1(env, embs, n=len(g), de_goal=final_goal, input_dim=input_dim):
    mat1 = np.zeros((n, int(input_dim)))
    for u in env.active:
        mat1[u, :-3] = embs[u]
        mat1[u, -3] = 1
        mat1[u, -1] = de_goal
    for u in env.possible_actions:
        mat1[u, :-3] = embs[u]
        mat1[u, -2] = 1
        mat1[u, -1] = de_goal
    return mat1


def get_embeds(g):
    d = {}
    for n in g.nodes:
        d[n] = str(n)
    g1 = nx.relabel_nodes(g, d)
    graph_model = DeepWalk(g1, num_walks=args.num_walks, walk_length=args.walk_len,
                           workers=args.cpu if args.cpu > 0 else cpu_count())

    graph_model.train(window_size=args.win, iter=args.emb_iters, embed_size=action_dim)
    embs = {}
    emb1 = graph_model.get_embeddings()
    for n in emb1.keys():
        embs[int(n)] = emb1[n]

    return embs


k = 10


def get_action(s, emb, nodes):
    q_vals = -10000.0
    node = -1
    for v in nodes:
        value, _ = acmodel.get_values2_(s[0], s[1], emb[v])
        if value > q_vals:
            q_vals = value
            node = v
    return node, q_vals


def get_action_curr1(s, emb, nodes):
    q_vals = -10000.0
    node = -1
    for v in nodes:
        value, _ = acmodel.get_values2(s[0], s[1], emb[v])
        if value > q_vals:
            q_vals = value
            node = v
    return node, q_vals


def get_action_curr2(s, emb, nodes):
    q_vals = -10000.0
    node = -1
    for v in nodes:
        _, value = acmodel.get_values2(s[0], s[1], emb[v])
        if value > q_vals:
            q_vals = value
            node = v
    return node, q_vals


node_attrs = make_const_attrs(g, input_dim-1)
final_goal = 0
tmp_goal = final_goal * np.ones([len(node_attrs), 1])
n_node_attrs = node_attrs
n_node_attrs = np.column_stack([node_attrs, tmp_goal])
# node_attrs[:, -1] = final_goal

n_iter = 0
HER = True
K_size = 1
shaped_reward = False
total_loss = []
avg_reward = []
total_rewards = []

noise_param = 0
# acmodel.eta = max(0.001, acmodel.eta * eta_decay)
args.epsilon = 0
# opts = [45.13,131.4,196.24,115.63,146.66]
# ch = [21.764,77.23,115.84,31.988,58.452]

rg2 = np.random.RandomState(100)
rg3 = np.random.RandomState(100)
# idx = 0
# g = graphs[idx]
# res = []
# for i in range(50):
#     e_seeds = list(rg2.choice(len(g), extra_seeds))

# damascus
seeds_list = [[1289, 527, 1344, 1180, 2009], [733, 1520, 1949, 2042, 1032], [1097, 2304, 2538, 40, 2550], [2419, 1552, 356, 239, 2443], [2102, 574, 1224, 974, 1713],
[77, 13, 2803, 409, 1406], [653, 1366, 2718, 89, 2700], [2591, 2873, 1828, 283, 93], [77, 2454, 2967, 2654, 2808], [395, 1692, 1240, 2063, 630],
[1095, 1419, 1681, 1454, 1159], [2763, 796, 2849, 1492, 2810], [2520, 44, 2693, 2052, 2119], [1880, 1842, 1388, 2338, 360], [2831, 2904, 2703, 2182, 1270],
[2645, 2794, 2710, 1547, 2672], [780, 2780, 480, 62, 1526], [2233, 1615, 2412, 232, 2797], [2361, 747, 2913, 2866, 2605], [1960, 2761, 2688, 914, 2583],
[1539, 2845, 1362, 1648, 1103], [785, 690, 921, 1390, 1258], [2833, 2512, 2138, 268, 1522], [286, 401, 1020, 2176, 671], [1381, 201, 1984, 1780, 380],
[2470, 2814, 534, 446, 355], [923, 1106, 2110, 1613, 506], [1387, 1072, 1397, 2013, 1773], [459, 2033, 1651, 917, 1953], [2655, 2784, 1765, 680, 346],
[1607, 2696, 2376, 412, 1279], [877, 1147, 985, 2254, 1237], [1037, 2977, 2564, 2775, 1374], [2588, 551, 347, 103, 777], [1287, 150, 2080, 1545, 1204],
[3020, 1732, 2206, 1478, 1006], [376, 2378, 2185, 2306, 1530], [2033, 3046, 627, 653, 436], [645, 2269, 2031, 340, 2992], [1834, 674, 2216, 46, 1952],
[1527, 350, 1210, 2861, 2066], [1458, 1580, 2367, 875, 2765], [2971, 595, 455, 2520, 1127], [655, 55, 1926, 1842, 2048], [2604, 2372, 1294, 2541, 117],
[1412, 902, 1225, 368, 2020], [2854, 2452, 1929, 552, 656], [2379, 314, 209, 1575, 2609], [127, 2443, 1230, 2410, 2895], [453, 432, 1340, 2874, 1384],
[515, 1228, 1495, 1169, 925], [453, 1714, 1587, 2053, 1271], [1900, 710, 189, 2280, 2762], [644, 918, 2835, 2645, 2761], [2680, 2339, 1291, 2601, 310],
[3026, 1043, 297, 3025, 2749], [2442, 1536, 830, 1621, 288], [1514, 2044, 269, 2294, 2940], [927, 1312, 1958, 2213, 207], [187, 1383, 2975, 1660, 1280],
[2329, 575, 1851, 2306, 2590], [2065, 1222, 959, 541, 1735], [1372, 1299, 391, 251, 938], [2647, 2873, 208, 2, 674], [58, 2698, 1103, 2976, 2885],
[2808, 2267, 144, 889, 63], [851, 2008, 1897, 560, 2033], [1691, 1389, 320, 1713, 1498], [265, 283, 2356, 2394, 2873], [1968, 113, 1386, 1047, 1433],
[659, 2608, 488, 2785, 587], [2653, 1310, 1823, 1180, 1125], [975, 2197, 2772, 227, 686], [1966, 1325, 1502, 852, 2196], [2481, 289, 2690, 1505, 1889],
[2028, 2386, 1229, 2114, 671], [1336, 1233, 2391, 465, 2430], [2103, 1889, 586, 15, 975], [848, 2086, 1463, 885, 1073], [2785, 901, 1992, 2589, 2323],
[2073, 2230, 1635, 2528, 1720], [1577, 346, 26, 1482, 1032], [2524, 160, 327, 2391, 1988], [2262, 468, 1135, 677, 111], [1603, 1550, 1701, 1849, 3022],
[620, 2995, 2226, 2517, 14], [888, 1460, 2974, 2129, 2570], [2327, 129, 540, 570, 2314], [1271, 523, 509, 2258, 1698], [2167, 651, 1354, 676, 575],
[2807, 316, 1027, 2354, 59], [1785, 162, 400, 1170, 61], [694, 2682, 956, 405, 2007], [1107, 583, 1168, 451, 727], [98, 1113, 2805, 2289, 656],
[1618, 2707, 2801, 2036, 2912], [50, 1512, 2184, 117, 1483], [600, 2280, 154, 2460, 2202], [2910, 2689, 1922, 2776, 1459], [1677, 2381, 2472, 390, 1895]]

# isereal
# seeds_list = [[1289, 3197, 527, 1344, 3441], [3195, 1180, 2009, 733, 1520], [1949, 2042, 1032, 1097, 2304], [2538, 40, 2550, 3492, 2419], [1552, 356, 239, 2443, 2102],
# [3416, 3435, 3194, 574, 3617], [1224, 974, 1713, 3126, 77], [3525, 13, 2803, 409, 1406], [653, 1366, 2718, 89, 2700], [3393, 2591, 2873, 1828, 283],
#  [3602, 93, 77, 2454, 2967], [2654, 2808, 395, 1692, 3146], [1240, 3465, 2063, 3602, 3536], [630, 1095, 3416, 1419, 1681], [1454, 1159, 2763, 796, 2849],
# [1492, 2810, 2520, 44, 2693], [2052, 2119, 3416, 1880, 1842], [1388, 3254, 2338, 360, 2831], [3533, 2904, 2703, 3069, 2182], [1270, 2645, 2794, 2710, 1547],
#  [2672, 780, 2780, 480, 62], [1526, 2233, 1615, 2412, 232], [3626, 2797, 2361, 747, 2913], [2866, 2605, 1960, 2761, 2688], [914, 2583, 1539, 2845, 3412],
# [1362, 1648, 3598, 3123, 1103], [3315, 785, 690, 3125, 921], [1390, 1258, 2833, 3488, 3409], [2512, 3113, 2138, 268, 1522], [286, 3281, 401, 1020, 3216],
# [2176, 671, 1381, 201, 1984], [1780, 380, 2470, 2814, 534], [3680, 3139, 446, 3167, 355], [923, 1106, 2110, 1613, 506], [1387, 1072, 1397, 2013, 1773],
# [459, 2033, 1651, 3542, 3493], [917, 1953, 2655, 2784, 1765], [3145, 680, 3429, 3499, 346], [1607, 2696, 2376, 412, 1279], [877, 3614, 1147, 985, 2254],
# [3537, 1237, 1037, 2977, 2564], [2775, 1374, 2588, 551, 347], [103, 777, 1287, 150, 2080], [1545, 1204, 3020, 1732, 2206], [1478, 1006, 376, 2378, 2185],
# [2306, 1530, 2033, 3046, 627], [3562, 653, 3531, 436, 645], [2269, 2031, 340, 2992, 1834], [674, 2216, 46, 1952, 1527], [350, 3158, 1210, 3525, 3448],
# [2861, 2066, 1458, 3568, 1580], [3457, 2367, 875, 2765, 3602], [2971, 595, 455, 2520, 1127], [655, 55, 1926, 3081, 1842], [2048, 2604, 2372, 1294, 2541],
# [117, 1412, 902, 3114, 1225], [3628, 368, 2020, 3546, 2854], [2452, 1929, 552, 656, 2379], [3495, 314, 209, 1575, 2609], [127, 2443, 1230, 2410, 2895],
# [3605, 453, 432, 1340, 2874], [3469, 1384, 515, 1228, 1495], [1169, 925, 453, 1714, 1587], [2053, 1271, 1900, 710, 189], [2280, 2762, 644, 918, 2835],
# [2645, 2761, 3363, 2680, 2339], [3184, 1291, 2601, 310, 3026], [1043, 297, 3025, 2749, 2442], [3108, 1536, 3323, 3230, 830], [1621, 288, 1514, 3688, 2044],
# [269, 2294, 2940, 927, 1312], [1958, 2213, 207, 187, 1383], [3604, 2975, 3161, 1660, 1280], [3424, 2329, 575, 3058, 1851], [2306, 2590, 2065, 1222, 959],
# [3158, 541, 3178, 1735, 1372], [1299, 3577, 391, 251, 3549], [938, 2647, 2873, 208, 2], [674, 58, 3532, 2698, 1103], [2976, 2885, 2808, 2267, 144],
# [3589, 889, 63, 851, 3494], [3257, 3207, 2008, 1897, 3622], [560, 2033, 1691, 1389, 320], [1713, 1498, 265, 283, 2356], [2394, 2873, 1968, 113, 1386],
# [1047, 1433, 659, 2608, 488], [2785, 3404, 587, 2653, 1310], [1823, 1180, 1125, 975, 3284], [2197, 2772, 227, 686, 1966], [1325, 1502, 852, 2196, 2481],
# [289, 2690, 1505, 1889, 3298], [2028, 2386, 3566, 1229, 2114], [3621, 671, 1336, 1233, 2391], [465, 2430, 2103, 1889, 586], [15, 975, 848, 2086, 1463],
# [885, 1073, 2785, 901, 1992], [2589, 3568, 2323, 2073, 3087], [2230, 1635, 2528, 1720, 1577], [346, 26, 1482, 1032, 2524], [160, 327, 2391, 1988, 2262]]


# isreal
seeds_list = [[1289, 3197, 527, 1344, 3441], [3195, 1180, 2009, 733, 1520], [1949, 2042, 1032, 1097, 2304], [2538, 40, 2550, 3492, 2419], [1552, 356, 239, 2443, 2102], [3416, 3435, 3194, 574, 3617], [1224, 974, 1713, 3126, 77], [3525, 13, 2803, 409, 1406], [653, 1366, 2718, 89, 2700], [3393, 2591, 2873, 1828, 283], [3602, 93, 77, 2454, 2967], [2654, 2808, 395, 1692, 3146], [1240, 3465, 2063, 3602, 3536], [630, 1095, 3416, 1419, 1681], [1454, 1159, 2763, 796, 2849], [1492, 2810, 2520, 44, 2693], [2052, 2119, 3416, 1880, 1842], [1388, 3254, 2338, 360, 2831], [3533, 2904, 2703, 3069, 2182], [1270, 2645, 2794, 2710, 1547], [2672, 780, 2780, 480, 62], [1526, 2233, 1615, 2412, 232], [3626, 2797, 2361, 747, 2913], [2866, 2605, 1960, 2761, 2688], [914, 2583, 1539, 2845, 3412], [1362, 1648, 3598, 3123, 1103], [3315, 785, 690, 3125, 921], [1390, 1258, 2833, 3488, 3409], [2512, 3113, 2138, 268, 1522], [286, 3281, 401, 1020, 3216], [2176, 671, 1381, 201, 1984], [1780, 380, 2470, 2814, 534], [3680, 3139, 446, 3167, 355], [923, 1106, 2110, 1613, 506], [1387, 1072, 1397, 2013, 1773], [459, 2033, 1651, 3542, 3493], [917, 1953, 2655, 2784, 1765], [3145, 680, 3429, 3499, 346], [1607, 2696, 2376, 412, 1279], [877, 3614, 1147, 985, 2254], [3537, 1237, 1037, 2977, 2564], [2775, 1374, 2588, 551, 347], [103, 777, 1287, 150, 2080], [1545, 1204, 3020, 1732, 2206], [1478, 1006, 376, 2378, 2185], [2306, 1530, 2033, 3046, 627], [3562, 653, 3531, 436, 645], [2269, 2031, 340, 2992, 1834], [674, 2216, 46, 1952, 1527], [350, 3158, 1210, 3525, 3448], [2861, 2066, 1458, 3568, 1580], [3457, 2367, 875, 2765, 3602], [2971, 595, 455, 2520, 1127], [655, 55, 1926, 3081, 1842], [2048, 2604, 2372, 1294, 2541], [117, 1412, 902, 3114, 1225], [3628, 368, 2020, 3546, 2854], [2452, 1929, 552, 656, 2379], [3495, 314, 209, 1575, 2609], [127, 2443, 1230, 2410, 2895], [3605, 453, 432, 1340, 2874], [3469, 1384, 515, 1228, 1495], [1169, 925, 453, 1714, 1587], [2053, 1271, 1900, 710, 189], [2280, 2762, 644, 918, 2835], [2645, 2761, 3363, 2680, 2339], [3184, 1291, 2601, 310, 3026], [1043, 297, 3025, 2749, 2442], [3108, 1536, 3323, 3230, 830], [1621, 288, 1514, 3688, 2044], [269, 2294, 2940, 927, 1312], [1958, 2213, 207, 187, 1383], [3604, 2975, 3161, 1660, 1280], [3424, 2329, 575, 3058, 1851], [2306, 2590, 2065, 1222, 959], [3158, 541, 3178, 1735, 1372], [1299, 3577, 391, 251, 3549], [938, 2647, 2873, 208, 2], [674, 58, 3532, 2698, 1103], [2976, 2885, 2808, 2267, 144], [3589, 889, 63, 851, 3494], [3257, 3207, 2008, 1897, 3622], [560, 2033, 1691, 1389, 320], [1713, 1498, 265, 283, 2356], [2394, 2873, 1968, 113, 1386], [1047, 1433, 659, 2608, 488], [2785, 3404, 587, 2653, 1310], [1823, 1180, 1125, 975, 3284], [2197, 2772, 227, 686, 1966], [1325, 1502, 852, 2196, 2481], [289, 2690, 1505, 1889, 3298], [2028, 2386, 3566, 1229, 2114], [3621, 671, 1336, 1233, 2391], [465, 2430, 2103, 1889, 586], [15, 975, 848, 2086, 1463], [885, 1073, 2785, 901, 1992], [2589, 3568, 2323, 2073, 3087], [2230, 1635, 2528, 1720, 1577], [346, 26, 1482, 1032, 2524], [160, 327, 2391, 1988, 2262], [3086, 3363, 468, 1135, 677], [111, 3646, 1603, 1550, 1701], [1849, 3022, 620, 3362, 2995], [2226, 2517, 14, 888, 1460], [2974, 2129, 2570, 2327, 129], [540, 570, 2314, 1271, 523], [509, 2258, 1698, 2167, 651], [3118, 1354, 676, 575, 2807], [316, 1027, 2354, 3059, 59], [1785, 162, 400, 1170, 61], [694, 2682, 956, 405, 3454], [2007, 1107, 583, 1168, 451], [3552, 727, 98, 1113, 2805], [2289, 656, 1618, 2707, 2801], [2036, 2912, 50, 1512, 2184], [117, 1483, 3272, 600, 2280], [154, 3408, 3190, 2460, 2202], [2910, 2689, 3419, 1922, 2776], [3388, 1459, 1677, 2381, 3261], [2472, 390, 1895, 1135, 1957], [1105, 70, 377, 2982, 765], [2155, 503, 2041, 3049, 478], [3330, 3402, 3384, 921, 1490], [3552, 2874, 1424, 3542, 1406], [2344, 2655, 3310, 1446, 3076], [407, 2106, 128, 422, 1908], [462, 3693, 1481, 3308, 2279], [1306, 849, 645, 2305, 2444], [3299, 86, 3443, 2545, 3072], [3271, 0, 826, 1562, 466], [967, 541, 3329, 1341, 2204], [2901, 1267, 1367, 2395, 626], [771, 3462, 740, 1634, 91], [3638, 1672, 1864, 522, 105], [1721, 340, 388, 3052, 1239], [1473, 1415, 3069, 1096, 1584], [1368, 3242, 2069, 1751, 1134], [2962, 1299, 3534, 1765, 2275], [160, 1687, 2775, 2991, 1327], [1826, 3296, 2403, 2833, 1513], [2400, 1120, 1391, 509, 402], [2884, 1154, 527, 3065, 2985], [1479, 1121, 1898, 3635, 1023], [2629, 1188, 714, 1052, 2359], [200, 1414, 272, 1919, 3587], [432, 2508, 1612, 1210, 1089], [2642, 2839, 2584, 1480, 1334], [2000, 627, 3093, 2742, 1375], [2358, 1027, 623, 610, 1433], [2729, 3086, 2761, 583, 166], [3505, 790, 1648, 1633, 1071], [2207, 2862, 2146, 389, 3470], [1868, 3344, 3292, 3604, 466], [2313, 677, 2658, 3334, 1694], [3651, 3015, 2246, 309, 2232], [1293, 1403, 1802, 1596, 639], [1662, 3617, 2176, 472, 2998], [2616, 676, 3031, 2926, 834], [1526, 3686, 2565, 2231, 2961], [1793, 713, 3507, 1997, 3213], [993, 1543, 509, 1818, 522], [1524, 1774, 3326, 1190, 1912], [3337, 2982, 1974, 2746, 673], [790, 3400, 3510, 1603, 3368], [2814, 3036, 943, 2791, 770], [3180, 535, 1261, 2463, 1857], [568, 2852, 1623, 2692, 1614], [3171, 3387, 726, 2334, 1964], [1639, 2341, 878, 2882, 3643], [2033, 547, 578, 1721, 3258], [2816, 1716, 3207, 3194, 1066], [2504, 1744, 2567, 1184, 2132], [3340, 2640, 2805, 3686, 100], [2016, 3017, 1345, 2597, 584], [1261, 1447, 3614, 2343, 3653], [167, 2034, 3680, 185, 2311], [1875, 2533, 2345, 2067, 2021], [1655, 1944, 15, 3286, 3188], [2209, 254, 357, 2114, 457], [3470, 1317, 664, 3577, 2131], [1043, 1087, 1871, 931, 2757], [2399, 2677, 2521, 635, 1571], [3166, 2915, 1782, 2370, 1275], [2308, 1663, 971, 2528, 1855], [652, 602, 2106, 1482, 911], [2300, 702, 2218, 2086, 2734], [2578, 2855, 259, 2593, 1335], [430, 2960, 1847, 1520, 2633], [2596, 2336, 1193, 119, 2307], [3303, 3084, 1888, 622, 49], [1245, 2547, 569, 1584, 487], [3189, 2524, 3366, 1730, 2023], [3069, 444, 630, 1340, 2401], [192, 868, 2789, 2114, 3058], [2056, 3149, 1718, 1895, 1045], [1729, 3540, 1336, 3312, 1823], [2599, 2090, 2676, 3105, 3320], [2062, 3307, 1100, 408, 3541], [3409, 3504, 2057, 3228, 3452], [3618, 716, 713, 2723, 597], [2759, 2185, 2845, 2060, 3077], [2632, 2597, 3365, 2345, 1993], [246, 292, 2310, 3349, 2097], [2113, 3253, 902, 3387, 386], [1453, 1633, 1508, 2269, 1138], [3248, 1326, 3288, 978, 1666], [2505, 1023, 1955, 1384, 2513], [3364, 156, 687, 1555, 1282], [2675, 2496, 3523, 757, 3235], [309, 135, 712, 2873, 2140], [3646, 933, 3259, 2860, 3638], [3095, 2694, 138, 3061, 884], [1221, 405, 2333, 2105, 2956], [1498, 100, 1262, 3219, 2678], [3303, 3091, 3652, 1015, 2953], [508, 141, 2197, 2731, 3056], [3695, 849, 3550, 2921, 3267], [1332, 3288, 682, 1860, 2405], [3449, 2180, 3414, 3635, 53], [1439, 206, 3399, 3457, 3309], [526, 2793, 1735, 1069, 443], [3454, 434, 2164, 630, 2358], [504, 293, 2563, 2241, 3348], [1034, 1086, 3683, 1078, 1811], [1410, 659, 1759, 1477, 488], [1267, 585, 2147, 250, 1140], [2015, 1537, 1373, 782, 3037], [910, 2784, 3080, 3314, 88], [3006, 644, 3540, 2441, 587], [2042, 1229, 286, 2297, 1714], [2301, 2350, 1103, 2458, 987], [759, 1606, 2958, 2951, 3255], [2043, 2188, 3027, 150, 748], [215, 1125, 2455, 2735, 3577], [198, 2525, 3207, 1083, 2203], [3564, 1700, 554, 1833, 1465], [921, 2332, 3167, 399, 2284], [2305, 1689, 1259, 2207, 2638], [645, 1550, 3054, 2340, 3551], [1312, 3060, 2334, 3578, 240], [2597, 2841, 1373, 3334, 1535], [2338, 919, 2492, 1915, 111], [509, 1694, 2799, 1273, 1016], [2217, 2455, 1321, 416, 1007], [2188, 1359, 1311, 3572, 3576], [3303, 3641, 2010, 391, 3024], [2583, 3540, 2569, 82, 1816], [940, 360, 2490, 3675, 1626], [1880, 2337, 3450, 1544, 2891], [858, 1969, 3268, 2467, 3509], [2837, 2860, 1421, 859, 972], [1250, 2031, 1962, 2139, 2047], [3078, 2422, 3177, 2286, 2388], [1108, 2736, 3278, 2424, 2284], [2284, 1609, 1130, 546, 1279], [3528, 1165, 1237, 136, 525], [2544, 122, 1945, 1239, 3271], [2874, 2468, 2720, 2695, 3252], [3025, 3428, 1228, 3102, 116], [76, 3394, 2586, 1649, 294], [2828, 2272, 1346, 3293, 2959], [2942, 425, 1412, 3123, 1784], [439, 1574, 3002, 377, 1208], [1197, 1519, 3352, 999, 2428], [1164, 3179, 1881, 2668, 2103], [1039, 41, 2215, 1955, 2011], [2370, 2797, 901, 1176, 1831], [3092, 2025, 3445, 1439, 859], [2192, 1702, 1435, 3164, 1491], [3112, 1844, 2721, 441, 2009], [1230, 1742, 52, 706, 1074], [358, 3354, 276, 30, 2348], [1647, 694, 2747, 1991, 3503], [498, 940, 3013, 397, 257], [2771, 712, 2530, 1973, 2372], [2389, 1901, 3587, 676, 2689], [554, 3608, 1743, 3610, 272], [2596, 2245, 3249, 722, 232], [960, 405, 3282, 3071, 802], [3599, 1466, 763, 2596, 1210], [479, 157, 438, 1706, 80], [3626, 1202, 2816, 542, 2626], [3669, 229, 1892, 1537, 2951], [2648, 172, 2361, 3437, 3367], [3540, 2509, 1876, 514, 2199], [3086, 293, 1405, 2381, 478], [2712, 1585, 1401, 164, 2070], [3131, 561, 532, 2139, 3695], [1509, 516, 569, 3419, 22], [2481, 2146, 1746, 1517, 3172], [3112, 536, 1387, 1128, 2031], [2320, 1862, 1904, 2029, 1795], [2882, 1857, 572, 944, 1516], [1471, 2697, 3042, 905, 3403], [2131, 3529, 2545, 2051, 2607], [2208, 1329, 1488, 3123, 1795], [387, 1710, 2493, 793, 2240], [894, 3016, 200, 467, 172], [3067, 2853, 634, 2032, 2597], [2495, 866, 3007, 1549, 2205], [3118, 580, 3219, 2913, 1820], [271, 2987, 139, 797, 3419], [2373, 995, 3126, 2218, 100], [3249, 2425, 2367, 1240, 14], [3148, 425, 1344, 836, 2202], [3624, 3299, 251, 2028, 2985], [3581, 1747, 2298, 1484, 2099], [2503, 1326, 1544, 1533, 1646], [1921, 1317, 2559, 3544, 1021], [2531, 600, 3144, 2169, 2057], [2747, 1640, 3298, 1435, 1697], [300, 1127, 654, 1203, 78], [3194, 780, 615, 2983, 2434], [266, 700, 718, 3213, 3578], [3623, 2948, 3070, 2801, 768], [478, 1975, 1619, 951, 2563], [999, 2738, 2980, 2388, 3645], [630, 1969, 2071, 489, 223], [817, 2549, 2041, 2687, 421], [3253, 3129, 1454, 921, 1185], [711, 137, 2081, 3462, 1568], [1183, 3362, 474, 3035, 1794], [214, 2248, 3605, 2151, 3356], [209, 411, 1575, 1313, 1201], [2653, 1810, 1173, 843, 641], [424, 3294, 1074, 3333, 2302], [2984, 3657, 486, 3165, 777], [523, 1183, 157, 1688, 238], [2162, 2535, 1171, 3, 2285], [3392, 1604, 1218, 3347, 3631], [2715, 2342, 2788, 2295, 3300], [1545, 3115, 2800, 2654, 644], [757, 791, 2124, 490, 1322], [1312, 2639, 1871, 2342, 2937], [229, 2552, 354, 1788, 3580], [2371, 3085, 2930, 2619, 2223], [2351, 269, 1675, 844, 636], [1956, 2547, 3443, 3511, 1926], [2235, 1004, 118, 1842, 2461], [1260, 673, 561, 3435, 749], [3347, 128, 532, 3612, 3479], [2521, 314, 2631, 580, 2672], [338, 1686, 1095, 2228, 793], [3664, 3039, 1063, 789, 664], [1401, 3145, 2031, 1193, 294], [285, 3059, 2741, 2896, 1513], [2356, 444, 1086, 3477, 3361], [2159, 2438, 3268, 2070, 2361], [722, 321, 611, 2834, 232], [3356, 2185, 2238, 634, 3383], [2093, 817, 2848, 1512, 1861], [2425, 185, 976, 2128, 1056], [541, 870, 179, 2994, 1203], [1782, 2123, 3667, 448, 1579], [1238, 2100, 3552, 1742, 816], [2041, 1904, 3528, 3600, 3125], [3602, 668, 2131, 1988, 2681], [2764, 266, 2579, 162, 894], [923, 2552, 3008, 3472, 2017], [3216, 2235, 2679, 665, 657], [3306, 3670, 2706, 2857, 378], [1819, 769, 2451, 1611, 2848], [171, 662, 2157, 1786, 2440], [735, 2912, 3183, 2921, 460], [1181, 2622, 2199, 2721, 1890], [2778, 1511, 1308, 1542, 3551], [670, 2055, 1333, 395, 3313], [2061, 127, 2974, 251, 431], [617, 3647, 2274, 1157, 3470], [575, 1590, 1390, 1474, 631], [1817, 3049, 2678, 3694, 2684], [1482, 575, 229, 1543, 3044], [967, 1916, 715, 1513, 1267], [2824, 3478, 914, 1923, 2027], [2373, 1737, 2433, 910, 1434], [3045, 1517, 2817, 1566, 639], [2097, 2957, 1001, 3212, 1963], [383, 3408, 3558, 1556, 3033], [416, 1791, 746, 458, 864], [2246, 439, 3667, 678, 552], [2954, 2510, 740, 2166, 2869], [594, 271, 2828, 319, 597], [1777, 3690, 38, 2299, 1760], [2172, 2606, 3123, 753, 2730], [1031, 2760, 3044, 941, 280], [1729, 583, 1047, 3008, 1690], [178, 3352, 3088, 619, 1609], [2424, 473, 71, 3580, 1623], [310, 3218, 328, 1089, 472], [3243, 2205, 301, 3673, 1325], [273, 34, 3303, 3125, 2061], [1541, 2912, 1505, 2542, 1509], [2035, 1379, 2494, 3181, 2174], [2832, 2959, 3110, 2437, 514], [2791, 2057, 1202, 1669, 583], [350, 112, 1754, 1280, 2173], [896, 2981, 894, 3395, 727], [2134, 1527, 1730, 3137, 1752], [598, 3129, 817, 2486, 2477], [2316, 787, 157, 1336, 2063], [487, 1724, 2, 284, 3259], [3207, 418, 2815, 1779, 1834], [2495, 3011, 2483, 1515, 1568], [2385, 2577, 431, 98, 3145], [438, 1564, 3044, 2185, 619], [161, 523, 6, 2441, 1837], [1438, 2619, 2997, 2728, 1008], [354, 131, 1830, 2348, 726], [3350, 2424, 225, 1795, 91], [3562, 132, 398, 2915, 1697], [1587, 741, 872, 1786, 580], [2516, 3550, 660, 1697, 1650], [3429, 2185, 2653, 2959, 3684], [2881, 1509, 508, 3398, 21], [356, 3450, 1678, 2780, 3422], [3494, 2, 289, 2252, 3250], [2140, 407, 1989, 3350, 2581], [1688, 2252, 2, 2869, 244], [221, 205, 3346, 396, 62], [1855, 2069, 2262, 1116, 763], [3562, 440, 267, 1496, 2301], [491, 2782, 2612, 102, 2561], [1435, 1857, 1507, 2154, 2752], [1481, 512, 904, 2859, 3031], [472, 3418, 2608, 3265, 2290], [1304, 2616, 3131, 303, 2187], [1773, 2550, 1103, 1993, 2246], [1224, 124, 170, 559, 2634], [425, 156, 415, 2209, 268], [159, 2266, 1664, 1822, 3277], [2384, 2644, 2788, 1468, 3665], [2331, 1816, 3201, 2212, 754], [2781, 1069, 800, 3493, 3690], [456, 1969, 866, 1230, 2028], [2385, 325, 2844, 1300, 618], [1536, 118, 1587, 1826, 3250], [414, 740, 2456, 1826, 3184], [1765, 3531, 550, 516, 1923], [2835, 1069, 2827, 1600, 438], [3307, 2955, 1188, 429, 367], [3158, 1301, 536, 2383, 2360], [2455, 3557, 2796, 312, 2909], [1237, 550, 2109, 1542, 1244], [808, 2663, 990, 3123, 1111], [2933, 307, 2191, 1623, 3101], [2625, 1230, 681, 3001, 2835], [3520, 399, 2019, 3380, 1372], [156, 3478, 369, 600, 2620], [3685, 2683, 3020, 1983, 723], [2170, 2299, 1728, 2691, 1401], [1088, 2003, 1817, 2536, 2279], [2521, 2655, 2778, 2322, 377], [1927, 1368, 1814, 1954, 749], [1461, 3542, 2828, 3220, 3517], [3339, 1253, 679, 3685, 2610], [677, 2005, 2987, 1652, 3693], [1445, 31, 824, 2877, 1048], [2109, 3221, 2287, 1328, 1644], [255, 1898, 3585, 2134, 697], [1618, 3407, 2234, 2424, 2786], [1131, 3316, 2466, 1278, 3514], [2710, 1030, 2352, 2725, 2565], [3445, 3668, 2794, 1960, 827], [412, 3002, 233, 2828, 1026], [2050, 1110, 2050, 400, 2744], [2108, 1677, 1485, 2978, 2835], [523, 3163, 3622, 465, 2802], [2831, 3567, 3195, 2275, 903], [3685, 751, 2845, 1209, 1009], [3026, 2750, 3445, 2382, 666], [1324, 1791, 3155, 1405, 1250], [1391, 2642, 851, 279, 62], [3673, 2445, 3237, 1378, 1549], [266, 591, 910, 1862, 3299], [3451, 2442, 847, 2286, 2165], [2110, 1119, 3667, 1559, 2130], [916, 744, 1821, 1008, 1914], [1994, 2148, 480, 846, 1420], [402, 2444, 975, 1038, 2520], [718, 260, 1162, 2612, 1346], [2709, 698, 3425, 1560, 1866], [2944, 1011, 2386, 2566, 3489], [2027, 1774, 1171, 2585, 2309], [789, 496, 2371, 733, 3154], [3531, 772, 2605, 1079, 320], [897, 3179, 1987, 674, 3693], [1398, 229, 2121, 1009, 3], [3153, 3118, 521, 927, 1851], [2313, 973, 982, 550, 327], [10, 847, 2403, 3173, 3500], [639, 2356, 1379, 1844, 84], [2319, 3477, 3394, 2983, 2042], [142, 2572, 1691, 2007, 2918], [2494, 3614, 967, 244, 2050], [1475, 1091, 2995, 2019, 3171], [2614, 1426, 3656, 126, 3173], [3394, 1912, 1945, 1562, 1000], [576, 812, 3054, 519, 2228], [1570, 683, 67, 3157, 2882], [2600, 3109, 1017, 1324, 1343]]


# plj
# seeds_list = [[1149, 527, 1147, 1180, 733], [1032, 1097, 256, 490, 40], [502, 371, 356, 239, 395], [54, 1146, 574, 1224, 974], [1078, 77, 13, 755, 409], [653, 670, 89, 652, 543], [825, 283, 93, 77, 406], [919, 606, 760, 395, 1098], [1240, 15, 630, 1095, 1159], [715, 796, 801, 762, 472], [44, 645, 4, 71, 1206], [290, 360, 783, 856, 655], [1021, 134, 597, 746, 662], [624, 780, 732, 480, 62], [185, 364, 232, 749, 313], [747, 865, 818, 557, 713], [640, 914, 535, 797, 1075], [1103, 785, 690, 1077, 921], [1258, 785, 464, 1065, 90], [268, 286, 1233, 401, 1020], [1168, 128, 671, 201, 380], [422, 766, 534, 1091, 446], [1119, 355, 923, 1106, 62], [506, 1072, 459, 917, 607], [736, 1097, 680, 346, 648], [328, 412, 877, 1147, 985], [206, 1237, 1037, 929, 516], [727, 540, 551, 347, 103], [777, 150, 32, 1204, 972], [158, 1006, 376, 330, 137], [258, 998, 627, 653, 436], [645, 221, 340, 944, 674], [168, 46, 350, 1110, 1210], [813, 18, 319, 875, 717], [923, 595, 455, 472, 1127], [655, 55, 1033, 0, 556], [324, 493, 117, 902, 1066], [1225, 368, 806, 404, 552], [656, 331, 314, 209, 561], [127, 395, 1230, 362, 847], [453, 432, 826, 515, 1228], [1169, 925, 453, 5, 710], [189, 232, 714, 644, 918], [787, 597, 713, 632, 291], [1136, 553, 310, 978, 1043], [297, 977, 701, 394, 1060], [1182, 830, 288, 269, 246], [892, 927, 165, 207, 187], [927, 1113, 281, 575, 1010], [258, 542, 17, 1222, 959], [1110, 541, 1130, 391, 251], [938, 599, 825, 208, 2], [674, 58, 650, 1103, 928], [837, 760, 219, 144, 889], [63, 851, 1209, 1159, 560], [320, 265, 283, 308, 346], [825, 113, 1047, 659, 560], [488, 737, 587, 605, 1180], [1125, 975, 1236, 149, 724], [227, 686, 852, 148, 433], [289, 642, 1250, 338, 1229], [66, 671, 1233, 343, 465], [382, 55, 586, 15, 975], [848, 38, 885, 1073, 737], [901, 541, 275, 25, 1039], [182, 480, 346, 26, 1032], [476, 160, 327, 343, 214], [1038, 468, 1135, 677, 111], [974, 620, 947, 178, 469], [14, 888, 926, 81, 522], [279, 129, 540, 570, 266], [523, 509, 210, 119, 651], [1070, 676, 575, 759, 316], [1027, 306, 1011, 59, 162], [400, 1170, 61, 694, 634], [956, 405, 1107, 583, 1168], [451, 727, 98, 1113, 757], [241, 656, 659, 753, 864], [50, 136, 117, 1224, 600], [232, 154, 1142, 412, 154], [862, 641, 728, 333, 1213], [424, 390, 1135, 1105, 70], [377, 934, 765, 107, 503], [1001, 478, 921, 826, 296], [607, 1262, 1028, 407, 58], [128, 422, 462, 1260, 231], [849, 645, 257, 396, 1251], [86, 497, 1024, 1223, 0], [826, 466, 967, 541, 156], [853, 347, 626, 771, 740], [91, 522, 105, 340, 388], [1004, 1239, 1021, 1096, 1194], [21, 1134, 914, 227, 160], [727, 943, 1248, 355, 785], [352, 1120, 509, 402, 836], [1154, 527, 1017, 937, 1121], [1023, 581, 1188, 714, 1052], [311, 200, 272, 432, 460], [1210, 1089, 594, 791, 536], [627, 1045, 694, 310, 1027], [623, 610, 681, 1038, 713], [583, 166, 790, 1071, 159], [814, 98, 389, 1244, 466], [265, 677, 610, 967, 198], [309, 184, 639, 128, 472], [950, 568, 676, 983, 878], [834, 517, 183, 913, 713], [1165, 993, 509, 522, 1190], [934, 698, 673, 790, 766], [988, 943, 743, 770, 1132], [535, 1261, 415, 568, 804], [644, 1123, 726, 286, 293], [878, 834, 547, 578, 1210], [768, 1159, 1146, 1066, 456], [519, 1184, 84, 592, 757], [100, 969, 549, 584, 1261], [295, 167, 185, 263, 485], [297, 19, 15, 1238, 1140], [161, 254, 357, 66, 457], [664, 83, 1043, 1087, 931], [709, 351, 629, 473, 635], [1118, 867, 322, 260, 971], [480, 652, 602, 58, 911], [252, 702, 170, 38, 686], [530, 807, 259, 545, 430], [912, 585, 548, 288, 1193], [119, 259, 1255, 1036, 622], [49, 1245, 499, 569, 487], [1141, 476, 1021, 444, 630], [353, 192, 868, 741, 66], [1010, 8, 1101, 1045, 551], [42, 628, 1057, 14, 1259], [1100, 408, 9, 1180, 716], [713, 675, 597, 711, 137], [797, 12, 1029, 584, 549], [297, 246, 292, 262, 49], [65, 1205, 902, 386, 221], [1138, 1200, 1240, 978, 457], [1023, 465, 156, 687, 627], [448, 757, 1187, 309, 135], [712, 825, 92, 933, 1211], [812, 1047, 646, 138, 1013], [884, 1221, 405, 285, 57], [908, 100, 1262, 1171, 630], [1255, 1043, 1015, 905, 508], [141, 149, 683, 1008, 849], [873, 1219, 1240, 682, 357], [132, 53, 206, 1261, 526], [745, 1069, 443, 434, 116], [630, 310, 504, 293, 515], [193, 1034, 1086, 1078, 659], [488, 585, 99, 250, 1140], [782, 989, 910, 736, 1032], [88, 958, 644, 393, 587], [1229, 286, 249, 253, 302], [1103, 410, 987, 759, 910], [903, 1207, 140, 979, 150], [748, 215, 1125, 407, 687], [198, 477, 1159, 1083, 155], [554, 921, 284, 1119, 399], [236, 257, 1259, 159, 590], [645, 1006, 292, 1012, 286], [240, 549, 793, 290, 919], [444, 111, 509, 751, 1016], [169, 407, 416, 1007, 140], [1255, 391, 976, 535, 521], [82, 940, 360, 442, 289], [843, 858, 1220, 419, 789], [812, 859, 972, 1250, 91], [1030, 374, 1129, 238, 340], [1108, 688, 1230, 376, 236], [236, 1130, 546, 1165, 1237], [136, 525, 496, 122, 1239], [1223, 826, 420, 672, 647], [1204, 977, 1228, 1054, 116], [76, 538, 294, 780, 224], [1245, 911, 894, 425, 1075], [439, 954, 377, 1208, 1197], [999, 380, 1164, 1131, 620], [55, 1039, 41, 167, 322], [749, 901, 1176, 1044, 859], [144, 1116, 1064, 673, 441], [1230, 52, 706, 1074, 358], [276, 30, 300, 694, 699], [498, 940, 965, 397, 257], [723, 712, 482, 324, 341], [676, 641, 554, 272, 548], [197, 1201, 722, 232, 960], [405, 1234, 1023, 802, 763], [548, 1210, 479, 157, 438], [80, 1202, 768, 542, 578], [229, 903, 600, 172, 313], [461, 514, 151, 1038, 293], [333, 478, 664, 164, 22], [1083, 561, 532, 91, 516], [569, 22, 433, 98, 1124], [1064, 536, 1128, 272, 834], [572, 944, 649, 994, 905], [83, 497, 3, 559, 160], [1075, 387, 445, 793, 192], [894, 968, 200, 467, 172], [1019, 805, 634, 549, 447], [866, 959, 157, 1070, 580], [1171, 865, 271, 939, 139], [797, 325, 995, 1078, 170], [100, 1201, 377, 319, 1240], [14, 1100, 425, 836, 154], [1251, 251, 937, 250, 51], [455, 511, 1021, 483, 600], [1096, 121, 9, 699, 1250], [300, 1127, 654, 1203, 78], [1146, 780, 615, 935, 386], [266, 700, 718, 1165, 900], [1022, 753, 768, 478, 951], [515, 999, 690, 932, 340], [630, 23, 489, 223, 817], [501, 639, 421, 1205, 1081], [921, 1185, 711, 137, 33], [1183, 474, 987, 214, 200], [103, 209, 411, 1201, 605], [1173, 843, 641, 424, 1246], [1074, 254, 936, 486, 1117], [777, 523, 1183, 157, 238], [114, 487, 1171, 3, 237], [1218, 667, 294, 740, 247], [1252, 1067, 752, 606, 644], [757, 791, 76, 490, 591], [294, 889, 229, 504, 354], [323, 1037, 882, 571, 175], [303, 269, 844, 636, 499], [187, 1004, 118, 413, 1260], [673, 561, 749, 128, 532], [473, 314, 583, 580, 624], [338, 1095, 180, 793, 991], [1063, 789, 664, 1097, 1193], [294, 285, 1011, 693, 848], [308, 444, 1086, 111, 390], [1220, 22, 313, 722, 321], [611, 786, 232, 137, 190], [634, 45, 817, 800, 377], [185, 976, 80, 1056, 541], [870, 179, 946, 1203, 75], [448, 1238, 52, 816, 1077], [668, 83, 633, 716, 266], [531, 162, 894, 923, 504], [960, 1168, 187, 631, 665], [657, 1258, 658, 809, 378], [769, 403, 800, 171, 662], [109, 392, 735, 864, 1135], [873, 460, 1181, 574, 151], [673, 730, 670, 7, 395], [13, 127, 926, 251, 431], [617, 226, 1157, 575, 631], [1001, 630, 636, 575, 229], [996, 967, 715, 776, 914], [325, 385, 910, 997, 769], [639, 49, 909, 1001, 1164], [383, 985, 416, 746, 458], [864, 198, 439, 678, 552], [906, 462, 740, 118, 821], [594, 271, 780, 319, 597], [38, 251, 124, 558, 1075], [753, 682, 1031, 712, 996], [941, 280, 583, 1047, 960], [178, 1040, 619, 376, 473], [71, 310, 1170, 328, 1089], [472, 1195, 157, 301, 273], [34, 1255, 1077, 13, 864], [494, 446, 1133, 126, 784], [911, 1062, 389, 514, 743], [9, 1202, 583, 350, 112], [125, 896, 933, 894, 727], [86, 1089, 598, 1081, 817], [438, 429, 268, 787, 157], [15, 487, 2, 284, 1211], [1159, 418, 767, 447, 963], [435, 337, 529, 431, 98], [1097, 438, 996, 137, 619], [161, 523, 6, 393, 571], [949, 680, 1008, 354, 131], [300, 726, 376, 225, 91], [132, 398, 867, 741, 872], [580, 468, 660, 137, 605], [911, 833, 508, 21, 356], [732, 2, 289, 204, 1202], [92, 407, 533, 204, 2], [821, 244, 221, 205, 396], [62, 21, 214, 1116, 763], [440, 267, 253, 491, 734], [564, 102, 513, 106, 704], [512, 904, 811, 983, 472], [560, 1217, 242, 568, 1083], [303, 139, 502, 1103, 198], [1224, 124, 170, 559, 586], [425, 156, 415, 161, 268], [159, 218, 1229, 336, 596], [740, 283, 1153, 164, 754], [733, 1069, 800, 456, 866], [1230, 337, 325, 796, 618], [118, 1202, 414, 740, 408], [1136, 550, 516, 787, 1069], [779, 438, 1259, 907, 1188], [429, 367, 1110, 536, 335], [312, 407, 748, 312, 861], [1237, 550, 61, 1244, 808], [615, 990, 1075, 1111, 885], [307, 143, 1053, 577, 1230], [681, 953, 787, 399, 156], [369, 600, 572, 635, 972], [723, 122, 251, 643, 1088], [488, 231, 473, 607, 730], [274, 377, 749, 780, 1172], [1253, 679, 562, 677, 939], [31, 824, 829, 1048, 61], [1173, 239, 255, 86, 697], [186, 376, 738, 1131, 418], [662, 1030, 304, 677, 517], [746, 827, 412, 954, 233], [780, 1026, 2, 1110, 2], [400, 696, 60, 930, 787], [523, 1115, 465, 754, 783], [1147, 227, 903, 751, 797], [1209, 1009, 978, 702, 334], [666, 1107, 1250, 594, 851], [279, 62, 397, 1189, 266], [591, 910, 1251, 394, 847], [238, 117, 62, 1119, 82], [916, 744, 1008, 100, 480], [846, 402, 396, 975, 1038], [472, 718, 260, 1162, 564], [661, 698, 896, 1011, 338], [518, 1171, 537, 261, 789], [496, 323, 733, 1106, 772], [557, 1079, 320, 897, 1131], [674, 229, 73, 1009, 3], [1105, 1070, 521, 927, 265], [973, 982, 550, 327, 10], [847, 355, 1125, 639, 308], [84, 271, 935, 142, 524], [870, 446, 967, 244, 2], [1091, 947, 1123, 566, 126], [1125, 1000, 576, 812, 1006], [519, 180, 683, 67, 1109], [834, 552, 1061, 1017, 818], [698, 891, 354, 284, 35], [630, 283, 432, 894, 903], [1241, 513, 1105, 758, 415], [1211, 768, 710, 933, 905], [714, 999, 36, 971, 518], [21, 876, 647, 637, 717], [157, 905, 721, 186, 1043], [1133, 416, 291, 69, 1002], [93, 177, 417, 1056, 377], [813, 1123, 670, 204, 982], [1182, 573, 994, 569, 217], [461, 120, 206, 1028, 1132], [899, 523, 301, 821, 162], [162, 1118, 247, 463, 962], [544, 133, 243, 476, 65], [967, 760, 1244, 583, 440], [1006, 362, 414, 415, 538], [1007, 221, 1032, 490, 585], [1137, 378, 930, 880, 716], [542, 477, 440, 750, 1048], [793, 1084, 238, 1218, 999], [159, 30, 724, 915, 754], [109, 448, 879, 260, 74], [344, 438, 646, 281, 384], [273, 257, 770, 932, 10], [996, 1212, 106, 618, 271], [434, 1170, 257, 760, 41], [1251, 184, 99, 518, 122], [823, 1210, 12, 1187, 846], [1218, 712, 524, 631, 203], [1155, 1068, 124, 721, 1087], [886, 1223, 252, 5, 50], [632, 319, 602, 656, 320], [227, 301, 217, 1194, 63], [257, 632, 523, 1252, 500], [862, 1110, 1011, 553, 75], [111, 745, 624, 813, 1194], [806, 274, 411, 1170, 947], [34, 685, 1041, 153, 509], [508, 71, 131, 956, 505], [755, 723, 804, 377, 810], [109, 14, 979, 120, 1256], [808, 215, 572, 315, 642], [639, 952, 1101, 777, 366], [923, 452, 211, 511, 906], [995, 1048, 318, 905, 123], [445, 1232, 1225, 89, 389], [1167, 116, 70, 625, 934], [516, 158, 93, 961, 930], [504, 590, 242, 650, 1188], [364, 406, 455, 1142, 665], [131, 950, 358, 1208, 920], [411, 184, 794, 752, 72], [724, 974, 1170, 1219, 415], [251, 587, 558, 867, 1063], [942, 191, 662, 350, 351], [913, 972, 223, 704, 305], [902, 299, 783, 1035, 642], [249, 660, 1233, 69, 1211], [1238, 603, 1140, 481, 714], [558, 538, 995, 595, 1233], [553, 44, 1025, 533, 581], [672, 7, 72, 733, 357], [4, 531, 1217, 1147, 790], [1083, 461, 261, 905, 62], [905, 337, 531, 809, 488], [244, 361, 299, 262, 300], [1201, 1101, 866, 819, 291], [155, 23, 135, 117, 269], [590, 450, 1136, 985, 1225], [666, 629, 1208, 525, 854], [1190, 1093, 457, 1098, 500], [836, 562, 184, 526, 1159], [899, 10, 436, 331, 11], [957, 663, 78, 1251, 1231], [1063, 71, 1238, 722, 202], [1226, 555, 49, 1100, 704], [1103, 918, 198, 61, 1070], [244, 227, 868, 305, 709], [659, 1139, 426, 981, 695], [157, 586, 297, 524, 679], [96, 804, 1194, 1067, 1219], [719, 129, 331, 954, 937], [48, 661, 288, 953, 1070], [1000, 173, 612, 141, 771], [1020, 352, 255, 1183, 505], [1028, 54, 168, 220, 471], [1008, 184, 829, 1121, 1138], [412, 1251, 801, 752, 1248], [939, 790, 665, 492, 554], [527, 743, 518, 1145, 863], [976, 980, 139, 599, 906], [160, 414, 1101, 1111, 701], [1071, 794, 832, 577, 1008], [874, 206, 152, 31, 956], [1246, 104, 802, 768, 848], [70, 981, 338, 704, 542], [1258, 407, 675, 881, 945], [239, 369, 317, 988, 935], [770, 824, 1173, 665, 243], [18, 1038, 374, 439, 1056], [649, 32, 833, 583, 375], [339, 1099, 156, 568, 789], [490, 985, 852, 305, 928], [1091, 645, 393, 1118, 402], [165, 284, 1019, 152, 49], [1159, 127, 383, 567, 1177], [1039, 789, 1180, 861, 965], [197, 754, 863, 693, 672], [654, 857, 1000, 439, 592], [248, 975, 1135, 182, 512], [224, 717, 157, 735, 609], [1262, 679, 1043, 583, 440], [760, 26, 199, 685, 743], [733, 295, 128, 1037, 859], [20, 115, 431, 146, 604], [434, 1118, 226, 169, 82], [1146, 423, 763, 930, 673], [55, 809, 119, 1163, 548], [48, 283, 462, 554, 1202], [534, 517, 26, 887, 387], [397, 919, 981, 10, 1249], [812, 857, 1005, 529, 451], [481, 623, 1165, 1238, 1163], [683, 1033, 1040, 691, 1045], [926, 379, 993, 806, 492], [965, 223, 1211, 1244, 828], [306, 1051, 89, 1142, 635], [361, 576, 811, 470, 638], [741, 238, 154, 723, 257], [1115, 818, 373, 779, 324], [78, 1039, 1191, 68, 1242], [289, 593, 717, 868, 1159], [224, 289, 940, 1038, 429], [735, 297, 575, 435, 371], [939, 492, 52, 268, 602], [192, 726, 933, 381, 944], [105, 204, 208, 1128, 106], [111, 947, 1168, 973, 948], [1008, 103, 147, 1247, 100], [836, 110, 955, 1230, 284], [639, 151, 140, 1036, 249], [839, 759, 964, 627, 507], [808, 1078, 746, 463, 501], [1046, 1208, 653, 271, 316], [596, 999, 545, 996, 190], [875, 1242, 941, 732, 1204], [54, 1224, 569, 254, 1014], [700, 830, 67, 429, 1220], [10, 746, 845, 498, 384], [423, 976, 783, 291, 417], [964, 983, 153, 735, 1179], [484, 782, 853, 957, 561], [277, 991, 491, 996, 1079], [221, 1198, 1060, 178, 773], [851, 314, 751, 323, 801]]

# bhp
seeds_list = [[1289, 1149, 527, 1344, 1393], [1147, 1180, 733, 1520, 1032], [1097, 256, 490, 40, 502], [1444, 371, 1552, 356, 239], [395, 54, 1368, 1387, 1146], [574, 1569, 1224, 974, 1078], [77, 1477, 13, 755, 409], [1406, 653, 1366, 670, 89], [652, 1345, 543, 825, 283], [1554, 93, 77, 406, 919], [606, 760, 395, 1098, 1240], [1417, 15, 1554, 1488, 630], [1095, 1368, 1419, 1681, 1454], [1159, 715, 796, 801, 1492], [762, 472, 44, 645, 4], [71, 1368, 1388, 1206, 290], [360, 783, 1485, 856, 655], [1021, 134, 1270, 597, 746], [662, 1547, 624, 780, 732], [480, 62, 1526, 185, 1615], [364, 232, 1578, 749, 313], [747, 865, 818, 557, 713], [640, 914, 535, 1539, 797], [1364, 1362, 1648, 1550, 1075], [1103, 1267, 785, 690, 1077], [921, 1390, 1258, 785, 1440], [1361, 464, 1065, 90, 268], [1522, 286, 1233, 401, 1020], [1168, 128, 671, 1381, 201], [380, 422, 766, 534, 1632], [1091, 446, 1119, 355, 923], [1106, 62, 1613, 506, 1387], [1072, 1397, 459, 1651, 1494], [1445, 917, 607, 736, 1097], [680, 1381, 1451, 346, 1607], [648, 328, 412, 1279, 877], [1566, 1147, 985, 206, 1489], [1237, 1037, 929, 516, 727], [1374, 540, 551, 347, 103], [777, 1287, 150, 32, 1545], [1204, 972, 158, 1478, 1006], [376, 330, 137, 258, 1530], [998, 627, 1514, 653, 1483], [436, 645, 221, 340, 944], [674, 168, 46, 1527, 350], [1110, 1210, 1477, 1400, 813], [18, 1458, 1520, 1580, 1409], [319, 875, 717, 1554, 923], [595, 455, 472, 1127, 655], [55, 1033, 0, 556, 324], [1294, 493, 117, 1412, 902], [1066, 1225, 1580, 368, 1498], [806, 404, 552, 656, 331], [1447, 314, 209, 1575, 561], [127, 395, 1230, 362, 847], [1557, 453, 432, 1340, 826], [1421, 1384, 515, 1228, 1495], [1169, 925, 453, 1587, 1684], [5, 1271, 710, 189, 232], [714, 644, 918, 787, 597], [713, 1315, 632, 291, 1136], [1291, 553, 310, 978, 1043], [297, 977, 701, 394, 1060], [1536, 1275, 1182, 830, 1621], [288, 1514, 1640, 269, 246], [892, 927, 1312, 165, 207], [187, 1383, 1556, 927, 1113], [1660, 1280, 1376, 281, 575], [1010, 258, 542, 17, 1222], [959, 1110, 541, 1130, 1372], [1299, 1529, 1664, 391, 251], [1501, 938, 599, 825, 208], [2, 674, 58, 1484, 650], [1103, 928, 837, 760, 219], [144, 1541, 889, 63, 851], [1446, 1209, 1159, 1574, 560], [1389, 320, 1498, 265, 283], [308, 346, 825, 113, 1386], [1047, 1433, 659, 560, 488], [737, 1356, 587, 605, 1310], [1180, 1125, 975, 1236, 149], [724, 227, 686, 1325, 1502], [852, 148, 433, 289, 642], [1505, 1250, 338, 1518, 1229], [66, 1573, 671, 1336, 1233], [343, 465, 382, 55, 586], [15, 975, 848, 38, 1463], [885, 1073, 1666, 737, 901], [541, 1520, 275, 25, 1039], [182, 1635, 480, 1577, 346], [26, 1482, 1032, 476, 160], [327, 343, 214, 1038, 1315], [468, 1135, 677, 111, 1598], [1603, 1550, 974, 620, 1314], [947, 178, 469, 14, 888], [1460, 926, 81, 522, 279], [129, 540, 570, 266, 1271], [523, 509, 210, 119, 651], [1070, 1354, 676, 575, 759], [316, 1027, 306, 1011, 59]]
seeds_list = [[1379, 474, 271, 1247, 924], [1242, 393, 1428, 1607, 1186], [992, 1142, 552, 1493, 474], [763, 979, 765, 742, 1214], [1168, 617, 775, 485, 1279], [994, 1030, 282, 1677, 1561], [203, 1465, 478, 288, 778], [1298, 259, 1533, 973, 1425], [43, 144, 786, 79, 699], [967, 587, 118, 1437, 1453], [518, 1313, 1486, 42, 515], [582, 1558, 1678, 516, 539], [264, 262, 538, 850, 1102], [1395, 1162, 234, 417, 725], [1650, 273, 808, 82, 980], [1147, 639, 158, 1672, 176], [1268, 816, 437, 294, 777], [828, 188, 663, 564, 224], [1484, 991, 1590, 1225, 667], [1510, 1258, 935, 215, 1270], [777, 1183, 1208, 824, 442], [1359, 1179, 1405, 638, 236], [881, 492, 1605, 1593, 663], [1247, 1125, 1276, 153, 1065], [1673, 1269, 194, 655, 374], [1217, 1310, 1624, 817, 1008], [168, 1675, 384, 186, 1110], [1573, 68, 1003, 658, 1289], [1465, 1166, 372, 101, 291], [1483, 361, 151, 1437, 97], [386, 86, 685, 341, 1492], [1521, 1084, 1176, 715, 567], [1601, 233, 1234, 51, 612], [1652, 1036, 804, 1354, 1120], [735, 1263, 1311, 1647, 1386], [612, 1091, 844, 170, 770], [1447, 96, 982, 719, 1010], [1035, 125, 981, 1563, 199], [919, 549, 822, 161, 134], [396, 164, 1244, 1158, 576], [135, 814, 1154, 1178, 966], [562, 1299, 367, 168, 1099], [1094, 969, 278, 208, 1360], [999, 1580, 1321, 1482, 411], [990, 562, 513, 374, 361], [405, 133, 937, 978, 138], [192, 1540, 1135, 232, 331], [1423, 310, 1163, 1028, 406], [1661, 774, 827, 231, 73], [1163, 786, 1123, 1216, 823], [64, 676, 48, 314, 39], [1061, 950, 732, 124, 5], [1434, 1039, 475, 1207, 1093], [1425, 1072, 1109, 1656, 1590], [455, 1392, 241, 791, 717], [174, 637, 615, 1432, 158], [1472, 375, 861, 1264, 548], [845, 1350, 674, 160, 517], [1373, 1424, 1135, 932, 1479], [1402, 1150, 638, 315, 331], [1236, 1313, 227, 661, 620], [1220, 1670, 914, 113, 483], [1279, 1136, 800, 1146, 1304], [776, 1457, 190, 1279, 1181], [1483, 595, 144, 566, 821], [1353, 621, 1490, 66, 612], [566, 577, 1517, 1580, 1247], [1278, 1451, 1021, 1136, 570], [1187, 1064, 405, 896, 1520], [277, 339, 1573, 1298, 1150], [610, 1363, 1444, 256, 808], [740, 28, 223, 537, 301], [167, 1306, 704, 289, 1439], [1063, 11, 755, 878, 1253], [349, 616, 198, 838, 1127], [1067, 830, 1251, 1319, 210], [1680, 277, 1574, 1085, 963], [1273, 469, 230, 1016, 1504], [64, 87, 508, 939, 462], [1280, 278, 1535, 850, 1177], [1624, 712, 1639, 220, 107], [1637, 1209, 118, 1269, 625], [776, 399, 1520, 322, 1074], [991, 131, 430, 27, 695], [1088, 1227, 149, 1533, 591], [750, 158, 720, 271, 363], [1671, 707, 463, 1493, 168], [1673, 236, 578, 465, 160], [179, 1043, 732, 1319, 951], [846, 1029, 1525, 1412, 976], [1214, 162, 179, 154, 787], [1126, 758, 1653, 1079, 1645], [1046, 1668, 892, 754, 818], [1548, 1492, 1241, 1348, 1292], [886, 584, 490, 920, 382], [169, 1562, 167, 47, 305], [995, 1490, 562, 724, 1466], [920, 153, 1597, 970, 494], [902, 332, 1121, 994, 828], [1030, 1680, 857, 124, 1507]]

#kcs
seeds_list = [[474, 271, 924, 393, 1186], [992, 1142, 552, 474, 763], [979, 765, 742, 1214, 1168], [617, 775, 485, 994, 1030], [282, 203, 478, 288, 778], [259, 973, 43, 144, 786], [79, 699, 967, 587, 118], [518, 42, 515, 582, 516], [539, 264, 262, 538, 850], [1102, 1162, 234, 417, 725], [273, 808, 82, 980, 1147], [639, 158, 176, 816, 437], [294, 777, 828, 188, 663], [564, 224, 991, 667, 935], [215, 777, 1183, 1208, 824], [442, 1179, 638, 236, 881], [492, 663, 1125, 153, 1065], [194, 655, 374, 1217, 817], [1008, 168, 384, 186, 1110], [68, 1003, 658, 1166, 372], [101, 291, 361, 151, 97], [386, 86, 685, 341, 1084], [1176, 715, 567, 233, 51], [612, 1036, 804, 1120, 735], [612, 1091, 844, 170, 770], [96, 982, 719, 1010, 1035], [125, 981, 199, 919, 549], [822, 161, 134, 396, 164], [1158, 576, 135, 814, 1154], [1178, 966, 562, 367, 168], [1099, 1094, 969, 278, 208], [999, 411, 990, 562, 513], [374, 361, 405, 133, 937], [978, 138, 192, 1135, 232], [331, 310, 1163, 1028, 406], [774, 827, 231, 73, 1163], [786, 1123, 1216, 823, 64], [676, 48, 314, 39, 1061], [950, 732, 124, 5, 1039], [475, 1207, 1093, 1072, 1109], [455, 241, 791, 717, 174], [637, 615, 158, 375, 861], [548, 845, 674, 160, 517], [1135, 932, 1150, 638, 315], [331, 227, 661, 620, 914], [113, 483, 1136, 800, 1146], [776, 190, 1181, 595, 144], [566, 821, 621, 66, 612], [566, 577, 1021, 1136, 570], [1187, 1064, 405, 896, 277], [339, 1150, 610, 256, 808], [740, 28, 223, 537, 301], [167, 704, 289, 1063, 11], [755, 878, 349, 616, 198], [838, 1127, 1067, 830, 210], [277, 1085, 963, 469, 230], [1016, 64, 87, 508, 939], [462, 278, 850, 1177, 712], [220, 107, 1209, 118, 625], [776, 399, 322, 1074, 991], [131, 430, 27, 695, 1088], [149, 591, 750, 158, 720], [271, 363, 707, 463, 168], [236, 578, 465, 160, 179], [1043, 732, 951, 846, 1029], [976, 1214, 162, 179, 154], [787, 1126, 758, 1079, 1046], [892, 754, 818, 886, 584], [490, 920, 382, 169, 167], [47, 305, 995, 562, 724], [920, 153, 970, 494, 902], [332, 1121, 994, 828, 1030], [857, 124, 284, 770, 777], [84, 577, 435, 360, 697], [1123, 488, 104, 1091, 108], [271, 2, 853, 152, 397], [759, 1066, 1134, 1024, 1169], [1124, 419, 181, 688, 113], [405, 1107, 1079, 138, 1057], [1117, 383, 794, 149, 835], [161, 891, 1100, 1003, 43], [1122, 529, 236, 750, 658], [988, 767, 1010, 577, 72], [1059, 290, 642, 193, 881], [686, 357, 425, 680, 330], [1139, 201, 196, 905, 276], [293, 1135, 1191, 873, 1009], [1028, 1075, 562, 1055, 793], [1094, 1158, 458, 577, 497], [499, 323, 977, 304, 408], [767, 983, 1131, 268, 429], [319, 1076, 179, 494, 527], [339, 97, 1027, 1096, 850], [983, 1190, 303, 1106, 129], [1004, 364, 347, 260, 243], [827, 857, 664, 930, 3], [3, 123, 25, 107, 971], [929, 1201, 102, 60, 233], [76, 619, 157, 1160, 1112], [449, 48, 209, 893, 1055]]

final_results = []
final_results2 = []
try:
    for epoch in range(1):
        for ep in range(100):
            time1 = time.time()
            idx = rg1.randint(len(graphs))
            env = envs[idx]
            g = graphs[idx]
            opt = opts[idx]
            change_score = ch[idx]
            goal_env = goal_envs[idx]
            print("Choosing %s" % (g_paths[idx]))

            res = []
            # if changeSeeds:
            #     e_seeds = list(rg2.choice(len(g), extra_seeds))
            # else:
            #     e_seeds = e_seeds_list[idx]
            # if (ep % 1000 == 0):
            #     e_seeds = e_seeds_list[idx]
            # e_seeds = seeds_list[ep]
            # goalss= [30,200,40,40,40,100,100,100,46,46,46,46]
            e_seeds = seeds_list[ep]
            final_goal = 60
            env.reset(seeds=e_seeds, goals=final_goal)

            domain_goal = True
            if domain_goal:
                t = env.state
                num_nodes = len(t)
                print('nodes number', len(t)-5)
                # final_goal = initial_goal = 0.5*((len(t)-5)/5)+5
                # final_goal = initial_goal = max(len(g)/(num_nodes),num_nodes)
                if num_nodes<20:
                    final_goal = initial_goal = 3*(num_nodes-5)
                elif num_nodes<40:
                    final_goal = initial_goal = num_nodes + len(g)/(5*num_nodes)
                else:
                    final_goal = initial_goal = num_nodes + len(g)/(5*num_nodes)

            final_goal = 80

            # for i in range(3):
            #     goal_env.reset(seeds=e_seeds)
            #     tmp_goal = goal_env.goal_setting2(budget, budget - 3)
            #     if tmp_goal > final_goal:
            #         final_goal = tmp_goal
            print('goal setting', final_goal)

            node_list = list(env.active.union(env.possible_actions))
            time2 = time.time()
            # print('time2-time1', time2 - time1)
            t = env.state
            # t0 = len(t)
            s_embs = get_embeds(env.sub)
            time22 = time.time()
            # print('time22-time2',time22-time2)
            if args.const_features:
                s = [n_node_attrs[node_list], env.state]
            else:
                s = [make_env_attrs_1(env=env, embs=s_embs, de_goal=final_goal, n=len(g))[node_list], env.state]

            print('Episode:', ep)
            print('Seeds:', e_seeds)
            tot_r = 0
            # tot_r1 = 0
            episode_experience = []
            # n_noed_list = []
            # n1_noed_list = []
            # n_noed_list.append([node_list, env.state])
            for stps in range(budget):
                time222 = time.time()
                possible_actions = [node_list.index(x) for x in env.possible_actions]

                # state_embed, _ = acmodel.get_node_embeddings(nodes_attr=s[0], adj=s[1], nodes=possible_actions)
                l = list(env.possible_actions)
                possible_actions_embed = [s_embs[x] for x in l]
                time3 = time.time()
                print('time3-time2', time3-time222)
                if rg1.rand() > args.epsilon and (acmodel.replay.size > batch_size or ep == 0):
                    # print('###################################################################################type', s[0].dtype)
                    if rg1.rand() > 0.5:
                        actual_action, q = get_action_curr1(s, s_embs, l)
                    else:
                        actual_action, q = get_action_curr2(s, s_embs, l)
                    proto_action = actual_action_embed = s_embs[actual_action]


                else:
                    actual_action = rg1.choice(list(env.possible_actions), 1)[0]
                    proto_action = actual_action_embed = s_embs[actual_action]
                time33 = time.time()
                print('time33-time3', time3-time3)
                res.append(actual_action)

                _, r, d, _, num_infl = env.step(actual_action, final_goal)
                # if num_infl < final_goal:
                #     r = 0
                # else:
                #     r = num_infl - final_goal
                time4 = time.time()
                print('time4-time33', time4-time33)
                g_n = num_infl
                if num_infl < final_goal:
                    r = -1
                else:
                    r = num_infl/final_goal
                node_list = list(env.active.union(env.possible_actions))
                t = env.state
                s_embs = get_embeds(env.sub)

                if args.const_features:
                    s1 = [n_node_attrs[node_list], env.state]
                else:
                    s1 = [make_env_attrs_1(env=env, embs=s_embs, de_goal=final_goal, n=len(g))[node_list], env.state]
                # n1_noed_list.append([node_list, env.state])


                # logging.debug('State: ' + str(state_embed))
                # logging.debug('Action:' + str(proto_action))
                # logging.info('State: ' + str(state_embed))
                # logging.info('Action:' + str(proto_action))

                # if last time step or explored entire graph
                if stps == budget - 1 or len(env.possible_actions) == 0:
                    env.step(-1, final_goal)
                    r = env.reward
                    num_infl = env.reward_
                    reward = num_infl
                    final_num_infl = num_infl
                    # shaped reward:
                    r = num_infl - final_goal

                    if num_infl < final_goal:
                        r = -1
                    else:
                        r = num_infl / final_goal

                    d = True
                if d:
                    s1[1] *= 0
                tot_r += r


                # sub = nx.from_numpy_matrix(t)
                # b,_,_ = influence(sub, sub)
                # t = len(env.state)
                # r1 = r + (1 / (len(g))) * (t - t0)
                # t0 = t


                reward = r
                time5 = time.time()
                print('time5-time4', time5-time4)
                # if d:
                #     r1 = r1 / opt
                #
                # tot_r1 += r1
                # tot_r1 = tot_r

                # tmp = s_embs[get_action(s1, s_embs, env.possible_actions)[0]]
                # TODO: TD Compute
                # td = acmodel.td_compute(s, actual_action_embed, r1, s1,
                #                         s_embs[get_action(s1, s_embs, env.possible_actions)[0]])
                # td = acmodel.td_compute(s, actual_action_embed, reward, s1,
                #                         s_embs[get_action(s1, s_embs, env.possible_actions)[0]])
                # print('########################################################################################################################################td',td)
                # num_infl = env.num_infl()
                # replay.add(s, actual_action_embed, reward, s1, s_embs[get_action(s1, s_embs, env.possible_actions)[0]],
                #            actual_action, td=np.abs(td))
                # episode_experience.append((s, actual_action_embed, reward, s1, final_goal,
                #                           s_embs[get_action(s1, s_embs, env.possible_actions)[0]], actual_action,
                #                            np.abs(td), num_infl))
                s = s1
                # if d:
                #     break
            # exp_size = len(episode_experience)
            # for stps in range(exp_size):
            #
            #     s, actual_action_embed, r1, s1, final_goal, s_embs, actual_action, td, num_infl = episode_experience[stps]
            #     replay.add(s, actual_action_embed, r1, s1, s_embs,
            #                            actual_action, td=np.abs(td))
            #
            #     if HER:
            #         for k in range(K_size):
            #             _, _, _, _, _, _, _, _, g_n = episode_experience[exp_size - 1]
            #             final_num_infl = g_n
            #             n_g = int(g_n)
            #             tmp_goal = n_g * np.ones([len(node_attrs), 1])
            #             if args.const_features:
            #                 n1_node_attrs = node_attrs
            #                 n1_node_attrs = np.column_stack([node_attrs, tmp_goal])
            #                 n_s = s
            #                 n_s1 = s1
            #                 n_s[0][:, 20] = n_g
            #                 n_s1[0][:, 20] = n_g
            #             else:
            #                 n_s = s
            #                 n_s1 = s1
            #                 n_s[0][:, -1] = n_g
            #                 n_s1[0][:, -1] = n_g
            #
            #             # if shaped_reward:
            #             #     n_r = num_infl / g_n if final else -np.sum(np.square(np.array(s1) == np.array(g_n)))
            #             # else:
            #             #     n_r = num_infl / g_n if final else -1
            #
            #             # if(stps == exp_size-1):
            #             #     n_r = num_infl/final_num_infl
            #             # elif(num_infl>final_num_infl):
            #             #     n_r = num_infl - final_num_infl
            #             # else:
            #             #     n_r = 0
            #             if num_infl < final_num_infl:
            #                 n_r = -1
            #             else:
            #                 # n_r = num_infl / final_num_infl
            #                 n_r = 1
            #             # n_r = num_infl - final_num_infl
            #
            #             # td = acmodel.td_compute(s, actual_action_embed, r1, s1,
            #             #                         s_embs[get_action(s1, s_embs, env.possible_actions)[0]])
            #             # td = acmodel.td_compute(n_s, actual_action_embed, n_r, n_s1,
            #             #                         s_embs[get_action(n_s1, s_embs, env.possible_actions)[0]])
            #             replay.add(n_s, actual_action_embed, n_r, n_s1, s_embs, actual_action, td=td)




                # s = s1
                # if d:
                #     break
            # mean_loss = []
            # if (ep == 0) or replay.size > batch_size:
            #     for i in range(5):
            #         acmodel.gradient_update_sarsa(batch_size=batch_size)
            #         # mean_loss.append(acmodel.loss_critic.clone().cpu().data.numpy())
            #         n_iter += 1
            #         if write:
            #             writer.add_scalar('CriticLoss with all optimization', acmodel.loss_critic.clone().cpu().data.numpy(), n_iter)
            #         if i==4:
            #             if write:
            #                 writer.add_scalar('Loss at the end of each episode', acmodel.loss_critic.clone().cpu().data.numpy(), n_iter)
            final_results.append(final_num_infl)
            final_results2.append(num_infl)
            torch.cuda.empty_cache()
            # if write:
            #     writer.add_scalar('CriticLoss', acmodel.loss_critic.clone().cpu().data.numpy(), ep + 1)
            # if write:
            #     writer.add_scalar('Mean Critic Loss of each episode:', mean_loss, ep+1)

            #
            # print('Critic Loss:', acmodel.loss_critic)
            # print('Action:', proto_action)
            print('Value:', q)
            # print('Env Reward:', r1)
            print('Reward:', tot_r)
            print('Chosen:', res, '\n')
            logging.info("Episode:" + str(ep) + 'Chosen:' + str(res))
            logging.info('Episode: ' + str(ep) + ' Reward: ' + str(tot_r))
            # logging.info('Episode: ' + str(ep) + 'Number of influenced: ' + str(final_num_infl))
            logging.info('Episode: ' + str(ep) + 'Number of n_g achieved: ' + str(g_n) + 'number:' + str(final_num_infl))
            logging.debug('Chosen:'+ str(res))
            # logging.debug('Critic Loss: ' + str(acmodel.loss_critic))
            # logging.debug('Final influenced number of nodes, num_infl:' + str(num_infl) + ' n_g:' + str(n_g) + 'final_influenced number' + str(final_num_infl))
            rws.append(tot_r)
            print('Final influenced number of nodes, num_infl:' + str(num_infl))
            print('final_influenced number', final_num_infl)
            print('time',time1,time2,time3,time4,time5)
            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!results:', final_results)
            # if write:
            #     writer.add_scalar('Reward', tot_r, ep + 1)
            #     writer.add_scalar('Influence', env.reward_, ep + 1)
            #     writer.add_scalar('Influence number:', num_infl, ep + 1)
                # writer.add_scalar('Norm Reward', tot_r1, ep + 1)


            gc.collect()

            # if ep % save_every == 0:
            #     torch.save(acmodel, 'HER_models/' + args.save_model + str(ep) + '.pth')


            # noise_param *= max(0.001, noise_decay_rate)
            # acmodel.eta = max(0.001, acmodel.eta * eta_decay)
            # args.epsilon = max(0.01, args.epsilon * args.eps_decay_rate)


        writer.close()

except KeyboardInterrupt:
    writer.close()